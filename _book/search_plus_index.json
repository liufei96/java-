{"./":{"url":"./","title":"Introduction","keywords":"","body":"使用 1. 本地环境安装 安装node.js npm install gitbook-cli -g 2. 运行项目 gitbook init 初始化一下项目 gitbook install -y 安装下插件 gitbook serve启动项目默认是4000端口 启动之后访问localhost:4000 powered by Gitbook该文件修订时间： 2020-07-18 10:09:47 "},"java/introduction-java.html":{"url":"java/introduction-java.html","title":"java高级内容","keywords":"","body":"java高级内容 powered by Gitbook该文件修订时间： 2020-07-28 23:19:41 "},"java/thread/线程池的使用.html":{"url":"java/thread/线程池的使用.html","title":"线程池","keywords":"","body":"并发包 1. 计数器（CountDownLatch） CountDownLatch 类位于java.util.concurrent包下，利用它可以实现类似计数器的功能。 比如有一个任务A，它要等待其他4个任务执行完毕之后才能执行，此时就可以利用CountDownLatch来实现这种功能了。CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 public static void main(String[] args) throws InterruptedException { // ThreadPoolExecutor newFixedThreadPool = new ThreadPoolExecutor(20, 20, // 0L, TimeUnit.MILLISECONDS, // new LinkedBlockingQueue()); ThreadPoolExecutor newFixedThreadPool = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.MILLISECONDS, new SynchronousQueue<>()); int size = 100; // 线程不安全的 final int[] count = {size}; AtomicInteger integer = new AtomicInteger(size); CountDownLatch countDownLatch = new CountDownLatch(size); for (int i = 0; i 结果： .... pool-1-thread-100,子线程开始执行... pool-1-thread-99,子线程开始执行... pool-1-thread-95,子线程开始执行... pool-1-thread-96,子线程开始执行... count:100 AtomicInteger:0 执行完成。...0 线程池 1. 什么是线程池 Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序 都可以使用线程池。在开发过程中，合理地使用线程池能够带来3个好处。 第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源， 还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用 线程池，必须对其实现原理了如指掌。 2. 线程池的作用 线程池是为突然大量爆发的线程设计的，通过有限的几个固定线程为大量的操作服务，减少了创建和销毁线程所需的时间，从而提高效率。 如果一个线程的时间非常长，就没必要用线程池了(不是不能作长时间操作，而是不宜。)，况且我们还不能控制线程池中线程的开始、挂起、和中止。 3. 线程的分类 powered by Gitbook该文件修订时间： 2020-07-30 22:52:02 "},"frame/spring/introduction-spring.html":{"url":"frame/spring/introduction-spring.html","title":"spring","keywords":"","body":"spring的介绍 Spring是一个开源框架，Spring是于2003年兴起的一个轻量级的Java开发框架，由Rod Johnson在其著作Expert One-On-One J2EE Development and Design中阐述的部分理念和原型衍生而来。 它是为了解决企业应用开发的复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许使用者选择使用哪一个组件，同时为J2EE应用程序开发提供集成的框架。Spring使用基本的JavaBean来完成以前只可能由EJB完成的事情。然而，Spring的用途不仅限于服务器端的开发。从简单性、可测试性和松耦合的角度而言，任何Java应用都可以从Spring中受益。Spring的核心是控制反转(IoC)和面向切面(AOP)。简单来说，Spring是一个分层的JavaSE/EEfull-stack(一站式)轻量级开源框架。 为什么说Spring是一个一站式的轻量级开源框架呢？EE开发可分成三层架构，针对JavaEE的三层结构，每一层Spring都提供了不同的解决技术。 WEB层：SpringMVC 业务层：Spring的IoC 持久层：Spring的JDBCTemplate(Spring的JDBC模板，ORM模板用于整合其他的持久层框架) powered by Gitbook该文件修订时间： 2020-07-15 23:53:43 "},"frame/spring/spring.html":{"url":"frame/spring/spring.html","title":"spring核心知识","keywords":"","body":"Spring的核心内容 spring的IOC spring的AOP 什么是AOP？ 关注点 切面 切入点 AOP底层的实现原理 什么是代理？ 静态代理 动态代理 ~ JDK动态代理 ~ CGLIB代理 CGLIB动态动态区别 AOP编程使用 ~ 注解版本： ~ XML版本 Spring事务使用 ~ 事务基本特征 ~ 事务控制的分类： 手写Spring的事务框架 手写Spring的注解事物 加上rollbackFor = Exception.class与不加的区别？ 注解 什么是注解 自定义注解 自定义事务注解 Spring事物传播行为 SpringMVC的执行流程 spring的IOC IOC（inverse of control）：控制反转 ，将对象创建的控制权交给IOC容器，应用程序中用到对象时，再从容器中获得。还有一个更确切的名字叫做DI（dependency injection）依赖注入，创建对象时根据对象之间的依赖关系根据配置自动注入依赖对象。 依赖注入的三种方式： （1）构造方法注入：即被注入对象可以通过在其构造方法中声明依赖对象的参数列表，让外部（通常是IOC容器）知道它需要哪些依赖对象，然后IOC容器会检查被注入对象的构造方法。 （2）setter方法注入：即当前对象只需要为其依赖对象所对应的属性添加setter方法，IOC容器通过此setter方法将相应的依赖对象设置到被注入对象的方式即setter方法注入。 （3）接口注入：接口注入有点复杂，被注入对象如果想要IOC容器为其注入依赖对象，就必须实现某个接口，这个接口提供一个方法，用来为被注入对象注入依赖对象，IOC容器通过接口方法将依赖对象注入到被注入对象中去。相对于前两种注入方式，接口注入比繁琐和死板，被注入对象就必须专声明和实现另外的接口。 默认情况下,IOC容器中创建的对象都是单列对象 bean标签上添加scope=”prototype”设置非单例对象。 默认情况下，创建容器对象时，会自动创建容器中的对象。单例的并且非懒加载对象（饿汉式，用不用先创建放在那）。只有在第一次从容器中获得该对象，才会创建。 spring的AOP 什么是AOP？ AOP：Aspect Oriented Programming 面向切面编程。 面向切面编程(也叫面向方面)：Aspect Oriented Programming(AOP),是目前软件开发中的一个热点。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 AOP是OOP的延续，是（Aspect Oriented Programming）的缩写，意思是面向切面（方面）编程。 主要的功能是：\\日志记录，性能统计，安全控制，事务处理，异常处理**等等。 主要的意图是：将日志记录，性能统计，安全控制，事务处理，异常处理等代码从业务逻辑代码中划分出来，通过对这些行为的分离，我们希望可以将它们独立到非指导业务逻辑的方法中，进而改变这些行为的时候不影响业务逻辑的代码。 可以通过预编译方式和运行期动态代理实现在不修改源代码的情况下给程序动态统一添加功能的一种技术。AOP实际是GoF设计模式的延续，设计模式孜孜不倦追求的是调用者和被调用者之间的解耦，AOP可以说也是这种目标的一种实现。 假设把应用程序想成一个立体结构的话，OOP的利刃是纵向切入系统，把系统划分为很多个模块（如：用户模块，文章模块等等），而AOP的利刃是横向切入系统，提取各个模块可能都要重复操作的部分（如：权限检查，日志记录等等）。由此可见，AOP是OOP的一个有效补充。 注意：AOP不是一种技术，实际上是编程思想。凡是符合AOP思想的技术，都可以看成是AOP的实现。 关注点 关注点：重复代码就叫做关注点； 切面 关注点形成的类，就叫切面(类)！ 面向切面编程，就是指 对很多功能都有的重复的代码抽取，在运行的时候往业务方法上动态植入“切面类代码”。 切入点 执行目标对象方法，动态植入切面代码。 可以通过切入点表达式，指定拦截哪些类的哪些方法； 给指定的类在运行的时候植入切面类代码。 AOP底层的实现原理 静态代理和动态代理 静态代理需要生成目标代理对象---不推荐 动态代理是不需要生成目标代理对象的 动态代理分为jdk动态代理和cglib jdk需要接口，动态dialing只需要自雷实现，基于java的反射 cglib是基于ASM字节码包装的一个类库。 ASM字节码： ASM是一个java字节码操纵框架，它能被用来动态生成类或者增强既有类的功能。ASM 可以直接产生二进制 class 文件，也可以在类被加载入 Java 虚拟机之前动态改变类行为。Java class 被存储在严格格式定义的 .class文件里，这些类文件拥有足够的元数据来解析类中的所有元素：类名称、方法、属性以及 Java 字节码（指令）。ASM从类文件中读入信息后，能够改变类行为，分析类信息，甚至能够根据用户要求生成新类。asm字节码增强技术主要是用来反射的时候提升性能的，如果单纯用jdk的反射调用，性能是非常低下的，而使用字节码增强技术后反射调用的时间已经基本可以与直接调用相当了 什么是代理？ 通过代理控制对象的访问,可以详细访问某个对象的方法，在这个方法调用处理，或调用后处理。既(AOP微实现) ,AOP核心技术面向切面编程。 应用场景： SpringAOP、事物原理、日志打印、权限控制、远程调用、安全代理 可以隐蔽真实角色 静态代理 由程序员创建或工具生成代理类的源码，再编译代理类。所谓静态代理也就是在程序运行前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。 传统的方式： public interface UserService { void add(); } public class UserServiceImpl implements UserService { public void add() { System.out.println(\"开启事务\"); System.out.println(\"添加数据\"); System.out.println(\"关闭事务\"); } } public class Main { public static void main(String[] args) { UserService userService = new UserServiceImpl(); userService.add(); } } 缺点： 每个方法中都需要加上开启事务和关闭事务，代码重复较多，维护起来麻烦 使用静态代理？ public interface UserService { void add(); } public class UserServiceImpl implements UserService { public void add() { System.out.println(\"添加数据\"); } } // 使用静态代理 public class UserServiceProxy implements UserService{ // 目标对象 private UserService target; public UserServiceProxy (UserService userService) { this.target = userService; } public void add() { System.out.println(\"开启事务\"); target.add(); System.out.println(\"结束事务\"); } } public class Main { public static void main(String[] args) { UserService userService = new UserServiceImpl(); UserServiceProxy proxy = new UserServiceProxy(userService); proxy.add(); } } 静态代理的缺点是： 针对每一个被代理类都需要创建一个代理对象 代理类和委托类实现相同的接口。 动态代理 1.代理对象,不需要实现接口 2.代理对象的生成,是利用JDK的API,动态的在内存中构建代理对象(需要我们指定创建代理对象/目标对象实现的接口的类型) 3.动态代理也叫做:JDK代理,接口代理 JDK动态代理 1) 原理：是根据类加载器和接口创建代理类（此代理类是接口的实现类，所以必须使用接口 面向接口生成代理，位于java.lang.reflect包下） 2) 实现方式： 通过实现InvocationHandler接口创建自己的调用处理器 IvocationHandler handler = new InvocationHandlerImpl(…); 通过为Proxy类指定ClassLoader对象和一组interface创建动态代理类Class clazz = Proxy.getProxyClass(classLoader,new Class[]{…}); 通过反射机制获取动态代理类的构造函数，其参数类型是调用处理器接口类型Constructor constructor = clazz.getConstructor(new Class[]{InvocationHandler.class}); 通过构造函数创建代理类实例，此时需将调用处理器对象作为参数被传入Interface Proxy = (Interface)constructor.newInstance(new Object[] (handler)); 缺点：jdk动态代理，必须是面向接口的，目标代理类必须实现接口。 public class InvocationHandlerImpl implements InvocationHandler { private Object target; public InvocationHandlerImpl(Object target) { this.target = target; } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"--- jdk动态代理：开启事务 ---\"); Object invoke = method.invoke(target, args); System.out.println(\"--- jdk动态代理：关闭事务 ---\"); return invoke; } public static void main(String[] args) { UserService userService = new UserServiceImpl(); InvocationHandlerImpl handler = new InvocationHandlerImpl(userService); ClassLoader classLoader = userService.getClass().getClassLoader(); Class[] interfaces = userService.getClass().getInterfaces(); UserService proxyInstance = (UserService) Proxy.newProxyInstance(classLoader, interfaces, handler); proxyInstance.add(); } } CGLIB代理 原理：利用asm开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。 使用cglib[Code Generation Library]实现动态代理，并不要求委托类必须实现接口，底层采用asm字节码生成框架生成代理类的字节码 cglib cglib 3.3.0 public class CglibProxy implements MethodInterceptor { private Object target; public Object getInstance(Object target) { this.target = target; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(target.getClass()); enhancer.setCallback(this); return enhancer.create(); } public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\"--- cglib 开启事务 ---\"); Object invoke = methodProxy.invoke(target, objects); System.out.println(\"--- cglib 关闭事务 ---\"); return invoke; } public static void main(String[] args) { CglibProxy cglibProxy = new CglibProxy(); UserService userService = (UserService) cglibProxy.getInstance(new UserServiceImpl()); userService.add(); } } CGLIB动态动态区别 java动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。 而cglib动态代理是利用asm开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。 Spring中。 1、如果目标对象实现了接口，默认情况下会采用JDK的动态代理实现AOP 2、如果目标对象实现了接口，可以强制使用CGLIB实现AOP 3、如果目标对象没有实现了接口，必须采用CGLIB库，spring会自动在JDK动态代理和CGLIB之间转换 JDK动态代理只能对实现了接口的类生成代理，而不能针对类 。 CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法 。 因为是继承，所以该类或方法最好不要声明成final ，final可以阻止继承和多态。 AOP编程使用 org.springframework spring-core 3.0.6.RELEASE org.springframework spring-context 3.0.6.RELEASE org.springframework spring-aop 3.0.6.RELEASE org.springframework spring-orm 3.0.6.RELEASE org.aspectj aspectjrt 1.6.1 aspectj aspectjweaver 1.5.3 因为这用的不是springboot项目，所以加上配置文件 注解版本： // 切面类 @Component @Aspect public class AopLog { // aop中的通知，前置通知，后置通知，运行通知，异常通知，环绕通知 @Before(\"execution(* com.liufei.spring.service.UserService.add(..))\") public void before() { System.out.println(\"前置通知\"); } @After(\"execution(* com.liufei.spring.service.UserService.add(..))\") public void after() { System.out.println(\"后置通知\"); } @AfterReturning(\"execution(* com.liufei.spring.service.UserService.add(..))\") public void afterReturning() { System.out.println(\"运行通知\"); } @AfterThrowing(\"execution(* com.liufei.spring.service.UserService.add(..))\") public void afterThrowing() { System.out.println(\"异常通知\"); } @Around(\"execution(* com.liufei.spring.service.UserService.add(..))\") public void around(ProceedingJoinPoint point) throws Throwable { System.out.println(\"环绕前通知\"); // 这里如果跑出异常，不会走下面的 point.proceed(); System.out.println(\"环绕后通知\"); } } public class AopMain { public static void main(String[] args) { ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"spring.xml\"); // 注意：UserServiceImp这个类要加上@Service UserService userService = (UserService) applicationContext.getBean(\"userServiceImpl\"); userService.add(); } } // 结果 环绕前通知 前置通知 添加数据 环绕后通知 运行结束后通知 后置通知 注意： 因为现在没有抛出异常，所以异常通知没有执行。 当add()方法有异常，并且没有捕获时。 环绕前通知; 前置通知; 异常通知; 后置通知; 如果add()方法中捕获了 XML版本 注意： UserService接口和UserServiceImpl实现类都不需要加上任何注解。 public class Aop2Main { public static void main(String[] args) { BeanFactory applicationContext = new ClassPathXmlApplicationContext(\"spring.xml\"); UserService userService = (UserService) applicationContext.getBean(\"userService\"); userService.add(); } } Spring事务使用 事务基本特征 （1）原子性（Atomicity） ​ 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 （2）一致性（Consistency） ​ 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 　　拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 （3）隔离性（Isolation） ​ 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 　　即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 　　关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 （4）持久性（Durability） ​ 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 ​ 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 事务控制的分类： 编程式事务 自己手动控制事务，就叫做编程式事务控制。 ​ Jdbc代码： ​ Conn.setAutoCommite(false); // 设置手动控制事务 ​ Hibernate代码： ​ Session.beginTransaction(); // 开启一个事务 ​ 【细粒度的事务控制： 可以对指定的方法、指定的方法的某几行添加事务控制】 ​ (比较灵活，但开发起来比较繁琐： 每次都要开启、提交、回滚.) 声明式事务 Spring提供了对事务的管理, 这个就叫声明式事务管理。 ​ Spring提供了对事务控制的实现。用户如果想用Spring的声明式事务管理，只需要在配置文件中配置即可； 不想使用时直接移除配置。这个实现了对事务控制的最大程度的解耦。 ​ Spring声明式事务管理，核心实现就是基于Aop。 ​ 【粗粒度的事务控制： 只能给整个方法应用事务，不可以对方法的某几行应用事务。】 ​ (因为aop拦截的是方法。) ​ Spring声明式事务管理器类： ​ Jdbc技术：DataSourceTransactionManager ​ Hibernate技术：HibernateTransactionManager 手写Spring的事务框架 连接数据库了，加上下面两个依赖 com.mchange c3p0 0.9.5.2 mysql mysql-connector-java 5.1.37 写个事务提交回滚的工具类 @Component public class TransactionUtils { @Autowired private DataSourceTransactionManager dataSourceTransactionManager; // 开启事务 public TransactionStatus begin() { TransactionStatus transaction = dataSourceTransactionManager.getTransaction(new DefaultTransactionAttribute()); return transaction; } // 提交事务 public void commit(TransactionStatus transactionStatus) { dataSourceTransactionManager.commit(transactionStatus); } // 回滚事务 public void rollback(TransactionStatus transactionStatus) { dataSourceTransactionManager.rollback(transactionStatus); } } 定义一个AOP切面 @Component @Aspect public class AopTransaction { @Autowired private TransactionUtils transactionUtils; @Around(\"execution(* com.liufei.spring.service.UserService.*(..))\") public void around(ProceedingJoinPoint point) throws Throwable { System.out.println(\"开启事务\"); TransactionStatus transactionStatus = transactionUtils.begin(); point.proceed(); System.out.println(\"提交事务\"); transactionUtils.commit(transactionStatus); } @AfterThrowing(\"execution(* com.liufei.spring.service.UserService.*(..))\") public void afterThrow() { System.out.println(\"回滚事务\"); TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); } } @Repository public class UserDao { @Autowired private JdbcTemplate jdbcTemplate; public void insert(String name) { String sql = \"insert into test(name) values(?)\"; int update = jdbcTemplate.update(sql, name); System.out.println(\"insert result:\" + update); } } public interface UserService { void insert(); } @Service public class UserServiceImpl implements UserService { @Autowired private UserDao userDao; public void insert() { userDao.insert(\"liufei\"); System.out.println(\"===============\"); userDao.insert(\"lihuihui\"); } } 注意：这个insert()里面不要进行异常捕获，否则事务回滚不了 public class AopMain { public static void main(String[] args) { ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"spring.xml\"); UserService userService = (UserService) applicationContext.getBean(\"userServiceImpl\"); userService.insert(); } 手写Spring的注解事物 使用@Transactional来实现系统自带的事物。 public interface UserService { void insert(); } @Service public class UserServiceImpl implements UserService { @Autowired private UserDao userDao; @Transactional public void insert() { userDao.insert(\"liufei\"); System.out.println(\"===============\"); userDao.insert(\"lihuihui\"); } } public class AopMain { public static void main(String[] args) { ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"spring.xml\"); UserService userService = (UserService) applicationContext.getBean(\"userServiceImpl\"); userService.insert(); } } 这样就能实现在方法上加上事物。 注意： insert方法中不要加上try {} catch。不然不会走到异常通知，事物不会回滚的 如果非要加上一场处理，则需要这样 @Transactional(rollbackFor = Exception.class) public void insert() { try { userDao.insert(\"liufei\"); int i = 1 / 0; System.out.println(\"===============\"); userDao.insert(\"lihuihui\"); } catch (Exception e) { e.printStackTrace(); TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); } } 加上rollbackFor = Exception.class与不加的区别？ 默认是不加的，不加，在@Transactional注解中如果不配置rollbackFor属性,那么事物只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事物在遇到非运行时异常时也回滚 注解 Jdk1.5新增新技术，注解。很多框架为了简化代码，都会提供有些注解。可以理解为插件，是代码级别的插件，在类的方法上写：@XXX，就是在代码上插入了一个插件。 注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用。 \\注解****分类：****内置****注解****(也****成为元注解* *jdk* *自带****注解****)****、****自定义注解（S****pring框架****）** 什么是注解 （1） @SuppressWarnings 再程序前面加上可以在javac编译中去除警告--阶段是SOURCE （2） @Deprecated 带有标记的包，方法，字段说明其过时----阶段是SOURCE （3）@Overricle 打上这个标记说明该方法是将父类的方法重写--阶段是SOURCE // 可以起到编译和检查的作用 @Override public String toString() { return null; } 如图所示：因为Object中没有add方法，此时在这个类中add()方法加上重写的方法，编译会报错。 // @Deprecated 案例 过时的方法 new Date().parse(\"\"); @SuppressWarnings案例演示 // 去除所有的警告 @SuppressWarnings({ \"all\" }) public void save() { java.util.List list = new ArrayList(); } 自定义注解 @Target({ElementType.TYPE, ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) public @interface AddAnnotation { String userName() default \"liufei\"; } // 使用 public class AnnotationTest { @AddAnnotation(userName = \"yiyang\") public void add() { } } @Target说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。 CONSTRUCTOR:用于描述构造器 FIELD:用于描述域 LOCAL_VARIABLE:用于描述局部变量 METHOD:用于描述方法 PACKAGE:用于描述包 PARAMETER:用于描述参数 TYPE:用于描述类、接口(包括注解类型) 或enum声明 读取注解 public class User { @AddAnnotation(userName = \"yiyang\") public void add() { } public void delete() { } public static void main(String[] args) throws Exception { Class aClass = Class.forName(\"com.liufei.spring.annotation.User\"); // 获取当前类中的方法（不包含继承父类） Method[] declaredMethods = aClass.getDeclaredMethods(); for (Method method: declaredMethods) { AddAnnotation annotation = method.getDeclaredAnnotation(AddAnnotation.class); if (annotation == null) { // 该方法上没有此注解 System.out.println(\"method:\" + method.getName() + \"没有此注解\"); continue; } System.out.println(\"userName:\" + annotation.userName()); } } } 自定义事务注解 --> public interface UserService { void insert(); } @Service public class UserServiceImpl implements UserService { @Autowired private UserDao userDao; @Override @ExtTransaction public void insert() { userDao.insert(\"liufei\"); System.out.println(\"===============\"); userDao.insert(\"lihuihui\"); } } @Target({ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) public @interface ExtTransaction { } @Component // @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE) public class TransactionUtils { @Autowired private DataSourceTransactionManager dataSourceTransactionManager; // 开启事务 public TransactionStatus begin() { System.out.println(\"开启事物\"); TransactionStatus transaction = dataSourceTransactionManager.getTransaction(new DefaultTransactionAttribute()); return transaction; } // 提交事务 public void commit(TransactionStatus transactionStatus) { System.out.println(\"提交事物\"); dataSourceTransactionManager.commit(transactionStatus); } // 回滚事务 public void rollback(TransactionStatus transactionStatus) { System.out.println(\"回滚事务\"); dataSourceTransactionManager.rollback(transactionStatus); } } @Component @Aspect public class AopTransaction { @Autowired private TransactionUtils transactionUtils; @Around(\"execution(* com.liufei.spring.service.*.*.*(..))\") public void around(ProceedingJoinPoint point) throws Throwable { // 1. 获取代理对象的方法，进而获取方法中的注解 ExtTransaction methodExtTransaction = getMethodExtTransaction(point); // 2. 判断方法中是否含有ExtTransaction注解 TransactionStatus transactionStatus = null; if (methodExtTransaction != null) { // 3. 如果含有，则开启事物 transactionStatus = transactionUtils.begin(); } // 4. 调用目标代理对象 point.proceed(); // 5. 判断方法上是否有注解：如果有则提交事物 if (transactionStatus != null) { transactionUtils.commit(transactionStatus); } } public ExtTransaction getMethodExtTransaction(ProceedingJoinPoint point) throws NoSuchMethodException { // 获取方法名称 String methodName = point.getSignature().getName(); // 获取目标对象 Class aClass = point.getTarget().getClass(); // 获取目标对象类型 Class[] par = ((MethodSignature) point.getSignature()).getParameterTypes(); // 获取目标对象方法 Method objMethod = aClass.getMethod(methodName, par); // 判断是否有自定义事务注解 ExtTransaction declaredAnnotation = objMethod.getDeclaredAnnotation(ExtTransaction.class); return declaredAnnotation; } } public class AopMain { public static void main(String[] args) { ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"spring.xml\"); UserService userService = (UserService) applicationContext.getBean(\"userServiceImpl\"); userService.insert(); } } 事务的回滚 @AfterThrowing(\"execution(* com.liufei.spring.service.*.*.*(..))\") public void afterThrow() { // 回滚事务 transactionUtils.rollback(); } /*** * @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE) 加上这个编程多列 * private TransactionStatus transaction; 把这个全局事务状态提取出来。 */ @Component @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE) public class TransactionUtils { @Autowired private DataSourceTransactionManager dataSourceTransactionManager; // 全局接收事务状态，可能会有线程安全问题。所以讲此类定义为多例 private TransactionStatus transaction; // 开启事务 public TransactionStatus begin() { System.out.println(\"开启事物\"); transaction = dataSourceTransactionManager.getTransaction(new DefaultTransactionAttribute()); return transaction; } // 提交事务 public void commit(TransactionStatus transactionStatus) { System.out.println(\"提交事物\"); dataSourceTransactionManager.commit(transactionStatus); } // 回滚事务 public void rollback() { if (transaction != null) { dataSourceTransactionManager.rollback(transaction); System.out.println(\"回滚事务\"); } } } 提醒自己： 1、如果事务中所有sql语句执行正确则需要自己手动提交commit；否则有任何一条执行错误，需要自己提交一条rollback，这时会回滚所有操作，而不是commit会给你自动判断和回滚。 2、新开一个事务会将该连接中的其他未提交的事务提交，相当于commit！ 3、事务既没有提交也没有回滚时连接断开数据库会自动回滚 4、事务中，update语句如果没有commit的话，你再重新执行update语句，就会等待锁定，当等待时间过长的时候，就会报ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction的错误。 Spring事物传播行为 Spring中事务的定义： Propagation（key属性确定代理应该给哪个方法增加事务行为。这样的属性最重要的部份是传播行为。）有以下选项可供使用： 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。（默认） PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。（就是如果外层没有事务，内层也不会执行事务） PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 案例： 当我们调用接口的时候，在调用接口前，需要写入记录，但是调用接口失败的时候，记录一样写入，而不是依赖后续的操作。 @Repository public class LogDao { @Autowired private JdbcTemplate jdbcTemplate; public void addLog(String name) { String sql = \"insert log(name) values(?)\"; int update = jdbcTemplate.update(sql, name); System.out.println(\"insert log result:\" + update); } } public interface LogService { void addLog(); } @Service public class LogServiceImpl implements LogService { @Autowired private LogDao logDao; @Override @Transactional public void addLog() { logDao.addLog(\"addLog\" + System.currentTimeMillis()); } } @Service public class UserServiceImpl implements UserService { @Autowired private UserDao userDao; @Autowired private LogService logService; @Override @Transactional public void insert() { logService.addLog(); userDao.insert(\"liufei\"); System.out.println(\"===============\"); int i = 1 / 0; userDao.insert(\"lihuihui\"); } } PROPAGATION_REQUIRED 默认事务是这个传播行为。如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 当执行insert()方法时，因为有一个事务，在执行到addLog()方法时，就不会新建一个事务，导致两个操作是由依赖关系的。 如果insert()后面出现异常，就会回滚，则此时添加日志操作也会被回滚。 修改事务的传播行为。 @Override @Transactional(propagation = Propagation.REQUIRES_NEW) public void addLog() { logDao.addLog(\"addLog\" + System.currentTimeMillis()); } 这样当插入user数据出现异常的时候，日志还是会正常写入的。 SpringMVC的执行流程 1）用户向服务器发送请求，请求被Spring前端控制Servlet（DispatcherServlet）捕获。\\（捕获）** （2）DispatcherServlet对请求URL进行解析，得到请求资源标志符（uri），然后根据该URI，调用handlerMapping，获得该handler配置的所有相关对象（包括handler对象以及handler对象对应的拦截器），最后以handlerExcutionChain对象的形式返回。\\(查找Handler)** （3）DispatcherServlet 根据获得的Handler，选择合适的HandlerAdapter,提取request中的数据模型，填充入Handler入参，开始执行Handler，Handler执行完成后，向DispatcherServlet返回一个ModelAndView对象。\\（执行handler）** （4）DipatcherServlet根据返回的ModelAndView，选择一个合适的ViewResolver（必须已经注入到Spring容器中的ViewResolver）（\\选择ViewResolver）** （5）通过ViewResolver结合Model和View，来渲染视图，DispatcherServlet将渲染的结果返回客户端。\\（渲染返回）** powered by Gitbook该文件修订时间： 2020-07-25 09:52:56 "},"frame/spring/手写spring的IOC容器.html":{"url":"frame/spring/手写spring的IOC容器.html","title":"手写spring的IOC容器","keywords":"","body":"手写spring的IOC容器 对应的是day0019的代码 XML技术 它是可扩展标记语言（Extensible Markup Language，简称XML），是一种标记语言。 XML 全称为可扩展的标记语言。主要用于描述数据和用作配置文件。 XML 文档在逻辑上主要由一下 5 个部分组成： XML 声明：指明所用 XML 的版本、文档的编码、文档的独立性信息 文档类型声明：指出 XML 文档所用的 DTD 元素：由开始标签、元素内容和结束标签构成 注释：以结束，用于对文档中的内容起一个说明作用 处理指令：通过处理指令来通知其他应用程序来处理非 XML 格式的数据，格式为 XML 文档的根元素被称为文档元素，它和在其外部出现的处理指令、注释等作为文档实体的子节点，根元素本身和其内部的子元素也是一棵树。 @残缺的孤独 20140101 北京海淀区 要么强大，要么听话 @残缺的孤独 20140102 北京朝阳区 在哭泣中学会坚强 *** 作用xml文件头部要写的话，说明了xml的版本和编码，utf-8一般是网络传输用的编码 XML解析方式？ Dom4j、Sax、Pull Dom4j和Sax区别？ dom4j不适合大文件的解析，因为它是一下子将文件加载到内存中，所以有可能出现内存溢出，sax是基于事件来对xml进行解析的，所以他可以解析大文件的xml，也正是因为如此，所以dom4j可以对xml进行灵活的增删改查和导航，而sax没有这么强的灵活性，所以sax经常是用来解析大型xml文件，而要对xml文件进行一些灵活（crud）操作就用dom4j。 使用dom4j解析xml public class XmlUtils { public static void main(String[] args) throws DocumentException { XmlUtils xmlUtils = new XmlUtils(); xmlUtils.test001(); } public void test001() throws DocumentException { SAXReader saxReader = new SAXReader(); Document read = saxReader.read(getClassPath(\"student.xml\")); // 获取根节点 Element rootElement = read.getRootElement(); getNodes(rootElement); } public static InputStream getClassPath(String xmlPath) { InputStream resourceAsStream = XmlUtils.class.getClassLoader().getResourceAsStream(xmlPath); return resourceAsStream; } public static void getNodes(Element rootElement) { System.out.println(\"获取当前名称:\" + rootElement.getName()); // 获取属性信息 List attributes = rootElement.attributes(); for (Attribute attribute : attributes) { System.out.println(\"属性:\" + attribute.getName() + \"---\" + attribute.getText()); } // 获取属性value String value = rootElement.getTextTrim(); if (!StringUtils.isEmpty(value)) { System.out.println(\"value:\" + value); } // 使用迭代器遍历,继续遍历子节点 Iterator elementIterator = rootElement.elementIterator(); while (elementIterator.hasNext()) { Element next = elementIterator.next(); getNodes(next); } } // 读取xml配置文件 public static List readerXml(String xmlPath) throws DocumentException { SAXReader saxReader = new SAXReader(); if (StringUtils.isBlank(xmlPath)) { new Exception(\"xml路径不能为空\"); } Document root = saxReader.read(getClassPath(xmlPath)); // 获取根节点 Element rootElement = root.getRootElement(); List elements = rootElement.elements(); if (elements == null || elements.isEmpty()) { return null; } return elements; } public static String findXmlByIDClass(List elements, String beanId) throws Exception { for (Element element : elements) { String beanIdValue = element.attributeValue(\"id\"); if (beanIdValue == null) { throw new Exception(\"使用该beanId没有查找到对应的元素\"); } if (!beanIdValue.equals(beanId)) { continue; } // 获取Class地址属性 String classPath = element.attributeValue(\"class\"); if (StringUtils.isNotBlank(classPath)) { return classPath; } } return null; } } XML与JSON区别 Xml是重量级数据交换格式，占宽带比较大。 JSON是轻量级交换格式，xml占宽带小。 所有很多互联网公司都会使用json作为数据交换格式 很多银行项目，大多数还是在使用xml。 SpringIOC的原理 使用反射机制+XML技术 手写SpringIOC XML版本 思想： 读取xml配置文件 使用beanId查找对应的class对象 获取class的信息地址，使用反射进行初始化 XmlUtils工具类 public class MyClassPathXmlApplicationContext { private String xmlPath; public MyClassPathXmlApplicationContext(String xmlPath) { this.xmlPath = xmlPath; } public Object getBean(String beanId) throws Exception { // 1.读取配置文件 List elements = XmlUtils.readerXml(xmlPath); if (elements == null) { throw new Exception(\"该配置文件没有子元素\"); } // 2.使用beanId查找对应的class对象 String xmlByIDClass = XmlUtils.findXmlByIDClass(elements, beanId); if (xmlByIDClass == null) { throw new Exception(\"没有找到改beanId对应的class\"); } // 3. 获取class的信息地址，使用反射进行初始化 Class aClass = Class.forName(xmlByIDClass); return aClass.newInstance(); } } public class Test001 { public static void main(String[] args) throws Exception { MyClassPathXmlApplicationContext applicationContext = new MyClassPathXmlApplicationContext(\"spring.xml\"); User user = (User) applicationContext.getBean(\"user\"); String userName = user.getUserName(); System.out.println(userName); } } 手写SpringIOC注解版本版本 思想：（扫包） 使用java反射机制扫包，获取当前包下的所有类 判断类上是否有我们定义的注解 然后将其存放到ConcurrentHashMap<>中，beanId为key，Class内容为value public class ClassUtil { /** * 取得某个接口下所有实现这个接口的类 */ public static List getAllClassByInterface(Class c) { List returnClassList = null; if (c.isInterface()) { // 获取当前的包名 String packageName = c.getPackage().getName(); // 获取当前包下以及子包下所以的类 List> allClass = getClasses(packageName); if (allClass != null) { returnClassList = new ArrayList(); for (Class classes : allClass) { // 判断是否是同一个接口 if (c.isAssignableFrom(classes)) { // 本身不加入进去 if (!c.equals(classes)) { returnClassList.add(classes); } } } } } return returnClassList; } /* * 取得某一类所在包的所有类名 不含迭代 */ public static String[] getPackageAllClassName(String classLocation, String packageName) { // 将packageName分解 String[] packagePathSplit = packageName.split(\"[.]\"); String realClassLocation = classLocation; int packageLength = packagePathSplit.length; for (int i = 0; i > getClasses(String packageName) { // 第一个class类的集合 List> classes = new ArrayList>(); // 是否循环迭代 boolean recursive = true; // 获取包的名字 并进行替换 String packageDirName = packageName.replace('.', '/'); // 定义一个枚举的集合 并进行循环来处理这个目录下的things Enumeration dirs; try { dirs = Thread.currentThread().getContextClassLoader().getResources(packageDirName); // 循环迭代下去 while (dirs.hasMoreElements()) { // 获取下一个元素 URL url = dirs.nextElement(); // 得到协议的名称 String protocol = url.getProtocol(); // 如果是以文件的形式保存在服务器上 if (\"file\".equals(protocol)) { // 获取包的物理路径 String filePath = URLDecoder.decode(url.getFile(), \"UTF-8\"); // 以文件的方式扫描整个包下的文件 并添加到集合中 findAndAddClassesInPackageByFile(packageName, filePath, recursive, classes); } else if (\"jar\".equals(protocol)) { // 如果是jar包文件 // 定义一个JarFile JarFile jar; try { // 获取jar jar = ((JarURLConnection) url.openConnection()).getJarFile(); // 从此jar包 得到一个枚举类 Enumeration entries = jar.entries(); // 同样的进行循环迭代 while (entries.hasMoreElements()) { // 获取jar里的一个实体 可以是目录 和一些jar包里的其他文件 如META-INF等文件 JarEntry entry = entries.nextElement(); String name = entry.getName(); // 如果是以/开头的 if (name.charAt(0) == '/') { // 获取后面的字符串 name = name.substring(1); } // 如果前半部分和定义的包名相同 if (name.startsWith(packageDirName)) { int idx = name.lastIndexOf('/'); // 如果以\"/\"结尾 是一个包 if (idx != -1) { // 获取包名 把\"/\"替换成\".\" packageName = name.substring(0, idx).replace('/', '.'); } // 如果可以迭代下去 并且是一个包 if ((idx != -1) || recursive) { // 如果是一个.class文件 而且不是目录 if (name.endsWith(\".class\") && !entry.isDirectory()) { // 去掉后面的\".class\" 获取真正的类名 String className = name.substring(packageName.length() + 1, name.length() - 6); try { // 添加到classes classes.add(Class.forName(packageName + '.' + className)); } catch (ClassNotFoundException e) { e.printStackTrace(); } } } } } } catch (IOException e) { e.printStackTrace(); } } } } catch (IOException e) { e.printStackTrace(); } return classes; } /** * 以文件的形式来获取包下的所有Class * * @param packageName * @param packagePath * @param recursive * @param classes */ public static void findAndAddClassesInPackageByFile(String packageName, String packagePath, final boolean recursive, List> classes) { // 获取此包的目录 建立一个File File dir = new File(packagePath); // 如果不存在或者 也不是目录就直接返回 if (!dir.exists() || !dir.isDirectory()) { return; } // 如果存在 就获取包下的所有文件 包括目录 File[] dirfiles = dir.listFiles(new FileFilter() { // 自定义过滤规则 如果可以循环(包含子目录) 或则是以.class结尾的文件(编译好的java类文件) public boolean accept(File file) { return (recursive && file.isDirectory()) || (file.getName().endsWith(\".class\")); } }); // 循环所有文件 for (File file : dirfiles) { // 如果是目录 则继续扫描 if (file.isDirectory()) { findAndAddClassesInPackageByFile(packageName + \".\" + file.getName(), file.getAbsolutePath(), recursive, classes); } else { // 如果是java类文件 去掉后面的.class 只留下类名 String className = file.getName().substring(0, file.getName().length() - 6); try { // 添加到集合中去 classes.add(Class.forName(packageName + '.' + className)); } catch (ClassNotFoundException e) { e.printStackTrace(); } } } } } @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface ExService { } public class ExClassPathXmlApplicationContext { // 扫包范围 private String packageName; private ConcurrentHashMap> beans = null; public ExClassPathXmlApplicationContext(String packageName) throws Exception { beans = new ConcurrentHashMap<>(); this.packageName = packageName; initBeans(); } // 初始化对象 public void initBeans() throws Exception { // 1.使用java反射机制扫包，获取当前包下的所有类 List> classes = ClassUtil.getClasses(packageName); // 2. 判断类上是否存在注入bean的注解 ConcurrentHashMap> classExistAnnotation = findClassExistAnnotation(classes); if (classExistAnnotation == null || classExistAnnotation.isEmpty()) { throw new Exception(\"改包下没有任何类的注解\"); } } public Object getBean(String beanId) throws Exception { if (StringUtils.isBlank(beanId)) { throw new Exception(\"beanId不能为空\"); } Class aClass = beans.get(beanId); if (aClass == null) { throw new Exception(\"class not found\"); } return aClass.newInstance(); } // 判断类上是否有注入bean的注解 public ConcurrentHashMap> findClassExistAnnotation(List> classes) { for (Class aClass: classes) { ExService exService = aClass.getDeclaredAnnotation(ExService.class); if (exService != null) { // bean的id是类名小写 String beanId = toLowerCaseFirstOne(aClass.getSimpleName()); beans.put(beanId, aClass); continue; } } return beans; } // 首字母转小写 public static String toLowerCaseFirstOne(String s) { if (Character.isLowerCase(s.charAt(0))) return s; else return (new StringBuilder()).append(Character.toLowerCase(s.charAt(0))).append(s.substring(1)).toString(); } } public class Test002 { public static void main(String[] args) throws Exception { ExClassPathXmlApplicationContext applicationContext = new ExClassPathXmlApplicationContext(\"com.liufei.service\"); UserService userServiceImpl = (UserService) applicationContext.getBean(\"userServiceImpl\"); userServiceImpl.add(); } } 手写@Resourse注解 思路： 接着上面的，将类注入到容器中，然后遍历每个类中属性，修改其默认值（初始化对象） /** * @Auther: liufei * @Date: 2020/07/26/11:54 上午 * @Description: */ @Target({TYPE, FIELD, METHOD}) @Retention(RUNTIME) public @interface ExResource { } public class ExClassPathXmlApplicationContext { // 扫包范围 private String packageName; private ConcurrentHashMap beans = null; public ExClassPathXmlApplicationContext(String packageName) throws Exception { beans = new ConcurrentHashMap<>(); this.packageName = packageName; initBeans(); // 初始化类中的属性 for(Map.Entry entry : beans.entrySet()) { Object value = entry.getValue(); initFields(value); } } // 初始化对象 public void initBeans() throws Exception { // 1.使用java反射机制扫包，获取当前包下的所有类 List> classes = ClassUtil.getClasses(packageName); // 2. 判断类上是否存在注入bean的注解 ConcurrentHashMap classExistAnnotation = findClassExistAnnotation(classes); if (classExistAnnotation == null || classExistAnnotation.isEmpty()) { throw new Exception(\"改包下没有任何类的注解\"); } } public void initFields(Object object) throws Exception { Field[] fields = object.getClass().getDeclaredFields(); for (Field field: fields) { ExResource exResource = field.getAnnotation(ExResource.class); if (exResource != null) { field.setAccessible(true); String beanId = field.getName(); Object newInstall = getBean(beanId); if (newInstall != null) { field.set(object, newInstall); } } } } public Object getBean(String beanId) throws Exception { if (StringUtils.isBlank(beanId)) { throw new Exception(\"beanId不能为空\"); } Object object = beans.get(beanId); return object; } // 判断类上是否有注入bean的注解 public ConcurrentHashMap findClassExistAnnotation(List> classes) throws IllegalAccessException, InstantiationException { for (Class aClass: classes) { ExService exService = aClass.getDeclaredAnnotation(ExService.class); if (exService != null) { // bean的id是类名小写 String beanId = toLowerCaseFirstOne(aClass.getSimpleName()); Object newInstance = aClass.newInstance(); beans.put(beanId, newInstance); continue; } } return beans; } // 首字母转小写 public static String toLowerCaseFirstOne(String s) { if (Character.isLowerCase(s.charAt(0))) return s; else return (new StringBuilder()).append(Character.toLowerCase(s.charAt(0))).append(s.substring(1)).toString(); } public static void main(String[] args) throws ClassNotFoundException { Class aClass = Class.forName(\"com.liufei.entity.User\"); System.out.println(aClass.getSimpleName()); } } ConcurrentMap存储的value变成Object，这样就可以实现单列 @ExService public class UserServiceImpl implements UserService { @ExResource private OrderService orderServiceImpl; @Override public void add() { orderServiceImpl.addOrder(); System.out.println(\"userService中的添加方法\"); } } public class Test002 { public static void main(String[] args) throws Exception { ExClassPathXmlApplicationContext applicationContext = new ExClassPathXmlApplicationContext(\"com.liufei\"); UserService userServiceImpl = (UserService) applicationContext.getBean(\"userServiceImpl\"); System.out.println(userServiceImpl); userServiceImpl.add(); } } powered by Gitbook该文件修订时间： 2020-07-26 14:04:13 "},"middleware/introdution-middle.html":{"url":"middleware/introdution-middle.html","title":"中间件","keywords":"","body":"中间件 powered by Gitbook该文件修订时间： 2020-07-28 23:22:00 "},"middleware/kafka.html":{"url":"middleware/kafka.html","title":"kafka","keywords":"","body":"1.2 安装与配置 1.2.1 java环境的安装 解压tar包 tar -zvf jdk-8u144-linux-i586.tar.gz # 修改环境变量 vim /etc/profile export JAVA_HOME=/usr/local/java/jdk1.8.0_144 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH export JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin export PATH=$PATH:${JAVA_PATH} # 重新加载配置文件 source /etc/profile # 验证结果 java -version 问题： 如果输入java -version出现的是这个 -bash: /usr/local/java/jdk1.8.0_144/bin/java: /lib/ld-linux.so.2: bad ELF interpreter: 没有那个文件或目录 则使用下面命令即可 yum install glibc.i686 1.2.2 ZooKeeper的安装 Zookeeper是安装Kafka集群的必要组建，Kafka通过Zookeeper来实施对原数据信息的管理，包括集群、主题、分区等内容。 同样在官网下载安装包到指定目录解压缩，步骤如下： Zookeeper官网：https://zookeeper.apache.org/ 修改Kafka的配置文件，首先进入安装路径conf目录，并将zoo_sample.cfg文件修改为zoo.cfg，并对核心参数进行配置。 文件内容如下： # The number of milliseconds of each tick # ZK服务器的心跳时间 tickTime=2000 # The number of ticks that the initial # synchronization phase can take # 投票选取新Leader的初始化时间 initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. # 数据目录 dataDir=/opt/apache-zookeeper-3.6.1-bin/data # 日志目录 dataLogDir=/opt/apache-zookeeper-3.6.1-bin/log # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 # # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance 启动Zookeeper命令：bin/zkServer.sh start [root@localhost bin]# zkServer.sh start -bash: zkServer.sh: command not found [root@localhost bin]# ./zkServer.sh start ZooKeeper JMX enabled by default Using config: /opt/apache-zookeeper-3.6.1-bin/bin/../conf/zoo.cfg Starting zookeeper ... STARTED 1.2.3 Kafka的安装和配置 官网内容：http://kafka.apache.org/ 下载解压成功后，开始启动 启动命令：bin/kafka-server.start.sh config/server.properties server.properties 需要关注以下几个配置 broker.id=0 表示broker的编号，如果集群中有多个broker，则每个broker的编号需要不同 listeners=PLAINTEXT://:9092 broker对外提供的服务入口地址 log.dirs=/opt/kafka_2.13-2.5.0/logs 设置存放消息日志文件的地址 zookeeper.connect=localhost:2181 kafka所需zookeeper集群地址。 验证结果 [root@localhost bin]# jps -l 54723 sun.tools.jps.Jps 53387 kafka.Kafka 103581 org.apache.zookeeper.server.quorum.QuorumPeerMain 1.2.4 kafka测试消息的生产与消费 首先创建一个主题 命令如下： bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic yiyang --partitions 2 --replication-factor 1 --zookeeper：制定了kafka所连接的zookeeper服务的地址 --topic：指定了所要创建主题的名称 --partitions：指定了分区个数 --replication-factor：指定了副本因子 --create：创建主题的动作命令 [root@localhost kafka_2.13-2.5.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic yiyang --partitions 2 --replication-factor 1 Created topic yiyang. 展示所有的主题 命令如下：bin/kafka-topics.sh --zookeeper localhost:2181 --list [root@localhost kafka_2.13-2.5.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --list yiyang 查看主题详情 命令如下：bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic yiyang [root@localhost kafka_2.13-2.5.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic yiyang Topic: yiyang PartitionCount: 2 ReplicationFactor: 1 Configs: Topic: yiyang Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: yiyang Partition: 1 Leader: 0 Replicas: 0 Isr: 0 启动消费端接收消息 命令：bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic yiyang --bootstrap-server 指定了连接kafka集群的地址 --topic 指定了消费端订阅的主题 地址在开一个终端 生产端发送消息 命令：bin/kafka-console-producer.sh --broker-list localhost:9092 --topic yiyang --broker-list 指定连接kafka集群的地址 --topic 指定了发送消息时的主题 [root@localhost bin]# ./kafka-console-producer.sh --broker-list localhost:9092 --topic yiyang >hello kafka > >nihao liufei >nihao yiyang > 此时在这个里面就可以输入消息 看下消费端的： [root@localhost kafka_2.13-2.5.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic yiyang hello kafka nihao liufei nihao yiyang 1.3. Java第一个程序 通过Java程序来进行kafka首发消息 1.3.1 准备 kafka自身提供的提供的java客户端来演示消息的收发，与kafka的java客户端相关的Maven依赖。 1.8 2.3.1 2.12 2.4.4 org.apache.kafka kafka-clients ${kafka.version} org.apache.kafka kafka_${scala.version} ${kafka.version} org.apache.zookeeper zookeeper org.slf4j slf4j-log4j12 log4j log4j org.apache.spark spark-streaming_${scala.version} ${spark.version} org.apache.spark spark-streaming-kafka-0-10_${scala.version} ${spark.version} org.apache.spark spark-sql_${scala.version} ${spark.version} org.apache.spark spark-sql-kafka-0-10_${scala.version} ${spark.version} 1.4 服务daunt的常用参数配置 1.4.1 zookeeper.connect 指明Zookeeper主机地址，如果zookeeper是集群则以逗号一个开，如： 172.16.103.128,172.16.103.129,172.16.103.130 1.4.2 listeners 监听列表，broker对外提供服务时绑定的IP和端口，多个以逗号隔开，如果监听器名称不时一个安全的协议，listener,security.protocol.map也必须设置。主机名称设置0.0.0.0绑定所有的接口，主机名称为空则绑定默认的接口。如：PLAINTEXT://myhost:9092,SSL://:9091 CLIENT:0.0.0.0:9092,REPLICATION://locahost:9093 1.4.3 broker.id broker的唯一标识符，如果不配置则自动生成。建议配置且一定要邦正集群中必须唯一，默认-1 1.4.4 log.dirs 日志数据存放的目录，如果没有配置则使用log.dir，建议配置此项配置。 1.4.5 message.max.bytes 服务器接收单个数据的最大大小，默认1000012约等于976.6KB。 第2章 生产者详解 2.1 消息发送 2.1.1 Kafka Java客户端数据生产流程解析 2.1.2 必要参数配置 2.1.3 发送类型 发送即忘记 Producer.send(record) 同步发送 // 通过send()发送完消息后返回一个Future对象，然后调用Future对象的get()方法等待kafka响应 // 如果kafka正常响应，返回一个RecordMetadata对象，该对象存储消息的偏移量 // 如果kafka发送错误，无法正常响应，就会抛出异常，我们便可以进行异常处理 producer.send(record).get(); 异步发送 producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (metadata != null) { System.out.println(\"TOPIC:\" + metadata.topic()); System.out.println(\"partition:\" + metadata.partition()); System.out.println(\"offset:\" + metadata.offset()); } } }); 2.1.4 序列化器 消息要到网络上进行传输，必须进行序列化，而序列化器的作用就是如此。 Kafka提供了默认的字符串序列化器(org.apache.kafka.common.serialization.StringSerializer)，还有整形(IntegerSerializer)和字节数组(BytesSerializer)序列化器，这些序列化器都实现了接口（org.apache.kafka.common.serialization.Serializer），基本上能够满足大部分场景的需求。 package com.yiyang.kafka.capter1; import org.apache.kafka.common.header.Headers; import org.apache.kafka.common.serialization.Serializer; import java.nio.ByteBuffer; import java.util.Map; public class UserSerializer implements Serializer { private String encoding = \"UTF-8\"; @Override public void configure(Map configs, boolean isKey) { } @Override public byte[] serialize(String topic, User data) { if (data == null) { return null; } byte [] name; byte [] address; try { if (data.getName() != null) { name = data.getName().getBytes(encoding); } else { name = new byte[0]; } if (data.getAddress() != null) { address = data.getAddress().getBytes(encoding); } else { address = new byte[0]; } ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + name.length + address.length); buffer.putInt(name.length); buffer.put(name); buffer.putInt(address.length); buffer.put(address); return buffer.array(); } catch (Exception e) { e.printStackTrace(); } return new byte[0]; } @Override public byte[] serialize(String topic, Headers headers, User data) { return new byte[0]; } @Override public void close() { } } 还需要一个反序列化 2.1.5 分区器 本身kafka有自己的分区策略，如果未指定，就会使用默认的分区策略。 kafka根据传递消息的key来进行区分，即hash(key)%numpartitions。如果Key相同的话，那么就会分配到同一分区 源码分析：org.apache.kafka.clients.producer.internals.DefaultPartitioner /** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { List partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) { int nextValue = nextValue(topic); List availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() > 0) { int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); } else { // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; } } else { // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; } } 自定义分区器，代码库 import org.apache.kafka.clients.producer.Partitioner; import org.apache.kafka.common.Cluster; import java.util.Map; public class MyDefaultPartition implements Partitioner { @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { // TODO 这里面写逻辑 return 0; } @Override public void close() { } @Override public void configure(Map configs) { } } // 使用自定义的分区 properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyDefaultPartition.class.getName()); 2.1.6 拦截器 Producer拦截器(interceptor)是相当于新的功能，它和consumer端的interceptor是在kafka0.10版本引入的，主要用于实现clients端的定制化控制逻辑。 生产者拦截器可以用来消息发送钱做一些准备工作。 使用场景： 1、按照某个规则过滤掉不符合要求的消息 2、修改消息的内容 3、统计类需求 见代码库：自定义拦截器 package com.yiyang.kafka.capter1; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import java.util.Map; public class MyProducerInterceptor implements ProducerInterceptor { private volatile long sendSuccess = 0; private volatile long sendFailed = 0; @Override public ProducerRecord onSend(ProducerRecord record) { String newValue = \"prefix-\" + record.value(); return new ProducerRecord(record.topic(), record.partition(),record.timestamp(),record.key(),newValue, record.headers()); } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { if (exception == null) { sendSuccess++; } else { sendFailed++; } } @Override public void close() { double successPercentage = sendSuccess / (sendFailed + sendSuccess); System.out.println(\"[INFO]发送成功率=\" + String.format(\"%f\", successPercentage * 100) + \"%\"); } @Override public void configure(Map configs) { } } 然后在使用的地方加上： properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, MyProducerInterceptor.class.getName()); 2.2 发送原理剖析 消息发送的过程中，设计到两个线程协同工作，主线程首先将业务数据封装成ProducerRecord对象，之后调用send()方法将消息放入RecordAccumulator（消息收集器，也可以理解为主线程与Sender线程直接的缓存区）中暂存，Sender线程负责将消息信息构成请求，并最终执行网络的I/O的线程，它从RecordAccumulator中取出消息并批量发送出去，需要注意的是，kafkaProducer是线程安全的，多个线程间可以共享使用同一个kafkaProducer对象 2.3 其他生产者参数 之前提及的默认三个客户端参数，大部分参数都是合理的默认值，一般情况下不需要修改它们 参考官网：http://kafka.apache.org/documentation/#producerconfigs 2.3.1 acks 这个参数用来指定分区中必须有多少个副本收到这条消息，之后生产者才会认为这条消息是写入成功的。acks是生产者客户端中非常重要的一个参数，它设置及到消息的可靠性和吞吐量之间的权衡。 ack=0，生产者在成功写入消息之前不会等待任何来自服务器的响应，如果出现问题生产者是感知不到的，消息就丢失。不过因为生产者不需要等待服务器的响应，所以它可以以网络能够支持的最大速度发送消息，从而达到很高的吞吐量。 acks=1，默认是1，只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应。如果消息无法达到首领节点（比如首领节点崩溃，新的首领还没有被选举出来），生产者会收到一个错误响应，为了避免数据丢失，生产者会重发消息。但是，这样还有可能导致数据丢失，如果收到写成功通知，此时首领节点还没来得及同步数据到fp;;pwer节点，首领节点崩溃，就会导致数据丢失。 ack=-1，只有当所有参与复制的节点收到消息时，生产者会收到一个来自服务器的成功响应，这种模式是最安全的，它可以保证不止一个服务器收到消息。 注意：acks参数配置的是一个字符串类型，而不是整数类型，如果配置为整数类型就会跑出以下异常： 2.3.2 retries 生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领）。这种情况下，如果达到了retries设置的次数，生产者会放弃重试并返回错误。默认情况下，生产者会在每次重试之间等待100ms，通过retry.backoff.ms参数来修改这个时间间隔。 2.3.3 batch.size 当有多个消息要被发送到同一分区时，生产者和UI把他们放在用一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算，而不是消息个数，当批次被填满，批次里的所有消息会被发送出去。不过生产者并不一定都会等到批次被填满才发送，半满的批次，甚至只包含一个消息的批次也可能被发送。所以就算把batch.size设置的很大，也不会造成延迟，只会占用更多的内存而已，如果设置的太小，生产者会因为频繁发送消息而增加一些额外的开销。 2.3.4 max.request.size 该参数用来控制身缠这发送的请求大小，他可以指定能发送的单个消息的最大值，也可以指单个请求里所有消息的总大小。broker对可接收的消息最大值也是有自己的限制（message.max.size），所以两边的配置最好的匹配，避免生产者发送的消息被broker拒绝 总结： 本章主要讲了生产者客户端的用法以及整体流程架构，主要内容包含配置采纳数的详解、消息的发送方式、序列化器、分区器、拦截器等，在实际使用中，kafka以及提供了良好的Java客户端支持，提高开发效率 第3章 消费者详解 tips 学完这一章你可以 深入学习Kafka数据消费大致流程 如何创建并使用Kafka消费者 Kafka消费者常用配置 3.1 概念入门 3.1.1 消费者和消费者组 kafka消费者是消费者组的一部分，当多个消费者形成一个消费者组来消费主体时，每个消费者会收到不同分区的消息。假设有一个T1主题，该主题有4个分区；同时我们有一个消费组G1，这个消费组只有一个消费者C1。那么消费者C1将会收到这4个分区的消息，如下所示： Kafka一个很重要的 特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。换句话说，每个应用都可以读到全量的消息。为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组G2，而这个消费组有两个消费者，那么会是这样的。 3.2 消息接收 见代码库： Properties properties = new Properties(); // 与KafkaProducer中设置保持一致 properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, UserDeSerializer.class.getName()); // 必填参数，该参数和kafkaProducer中的相同，制定连接的Kafka集群所需的Broker地址清单，可以设置一个或多个 properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_LIST); // 消费者隶属于的消费组，默认为空，如果设置为空，则会跑出异常，这个参数要设置成具有一定业务含义的名称 properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID); // 指定KafkaConsumer对应的客户端Id，默认为空，如果不设置kafkaConsumer会自动生成一个非空的字符串 properties.put(\"client.id\", \"consumer.client.id.demo\"); KafkaConsumer consumer = new KafkaConsumer(properties); consumer.subscribe(Collections.singletonList(TOPIC_NAME)); while (true) { ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord record : records) { System.out.println(\"消费的消息：\" + record.value()); } } 3.2.2 订阅主题和分区 创建完消费者后我们便可以订阅主题了，只需要通过调用subscrib()方法即可，这个方法接收一个主题列表 KafkaConsumer consumer = new KafkaConsumer(properties); consumer.subscribe(Collections.singletonList(TOPIC_NAME)); 另外，我们也可以使用正则表达式来匹配多个主题，而且订阅之后又有匹配新的主题，那么这个消费组会立即对其 进行消费。正则表达式在连接Kafka与其他系统时非常有用。比如订阅所有的测试主题： consumer.subscribe(Pattern.compile(\"yiyang*\")); 指定订阅的分区 // 指定订阅的分区 consumer.assign(Arrays.asList(new TopicPartition(\"topic226\", 0))); 3.2.2 反序列化 // 与KafkaProducer中设置保持一致 properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); 3.2.3 位移提交 对于Kafka中的分区而言，它的每一条消息都有位移的offset，用来表示消息在分区的位置。 当我们调用poll()时，该方法会返回我们没有消费的消息。当消息从broker返回消费时，broker并不跟踪这些消息是否被消费者接收到；Kafka让消费者自身来管理消费的位移，并向消费者提供更新位移的接口，这种更新位移的方式称之为提交（commit）。 重复消费 消息丢失 自动提交 这种方式让消费者来管理位移，应用本身不需要显示操作。当我们将enable.auto.commit设置为true，那么消费者会在poll方法调用后每隔5秒（由auto.commit.interval.ms指定）提交一次位移。和很多其他操作一样，自动提交也是由poll()方法来驱动；在调用poll()时，消费者判断是否到达提交时间，如果是则提交上一次poll返回的最大位移。 需要注意到，这种方式可能会导致消息重复消费。假如，某个消费者poll消息后，应用正在处理消息，在3秒后Kafka进行了重平衡，那么由于没有更新位移导致重平衡后，这部分消息重复消费 同步提交 package com.yiyang.kafka.capter1; import org.apache.kafka.clients.consumer.*; import org.apache.kafka.common.TopicPartition; import org.apache.kafka.common.serialization.StringDeserializer; import java.util.Arrays; import java.util.List; import java.util.Properties; public class CheckOffsetAndCommit { private static final String BROKER_LIST = \"yiyang:9092\"; private static final String TOPIC_NAME = \"yiyang\"; private static final String GROUP_ID = \"groupId.demo\"; public static Properties getProperties() { Properties properties = new Properties(); properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_LIST); properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID); properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // 关闭自动提交 properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); return properties; } public static void main(String[] args) { Properties properties = getProperties(); KafkaConsumer consumer = new KafkaConsumer(properties); TopicPartition tp = new TopicPartition(TOPIC_NAME, 0); consumer.assign(Arrays.asList(tp)); long lastConsumerOffset = -1; while (true) { ConsumerRecords records = consumer.poll(1000); if (records.isEmpty()) { break; } List> partitionsRecords = records.records(tp); lastConsumerOffset = partitionsRecords.get(partitionsRecords.size() - 1).offset(); // 同步提交消费位移 consumer.commitAsync(); System.out.println(\"消费的内容：\" + partitionsRecords.get(partitionsRecords.size() - 1).value()); } System.out.println(\"consumer offset is \" + lastConsumerOffset); OffsetAndMetadata offsetAndMetadata = consumer.committed(tp); System.out.println(\"commited offset is \" + offsetAndMetadata.offset()); long position = consumer.position(tp); // 下次消费的位置 System.out.println(\"the offset of the next record is \" + position); } } 异步提交 手动提交有一个缺点，那就是当发起提交调用时应用汇阻塞。当然我们可以减少手动提交的频率，但这个会增加消息重复的概率（和自动提交一样）。另外一个解决办法是，使用一部提交的API 见代码： 但是异步提交也有个缺点，那就是如果服务器返回提交失败，异步提交不会进行重试。相比较起来，同步提交会进行重试直到成功或者最后跑出异常给应用。异步提交没有实现重试是因为，如果同时存在多个异步提交，进行重试可能导致位移覆盖。举个例子，假如我们发起了一个异步提交commitA，此时的提交位移为2000，随后又发起了一个异步提交commitB位移为3000；commitA提交失败但commitB提交成功，此时commitA进行重试并成功的话，会将实际上将已经提交的位移从3000回滚到2000，导致消费重复消费。 异步回调： Properties properties = getProperties(); KafkaConsumer consumer = new KafkaConsumer(properties); consumer.subscribe(Arrays.asList(TOPIC_NAME)); try { while (running.get()) { ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord record : records) { } // 异步调用 consumer.commitAsync(new OffsetCommitCallback() { @Override public void onComplete(Map offsets, Exception exception) { if (exception == null) { System.out.println(offsets); } else { log.error(\"fail to commit offsets {}\", offsets, exception); } } }); } } finally { consumer.close(); } 3.2.4 指定位移消费 到目前为止，我们知道消息的拉取是根据poll()方法中的逻辑来处理的，但是这个方法对于普通开发人员来说就是个黑盒处理，无法精确掌握其消费的起始位置。 seek()方法正好提供了这个功能，让我们得以追踪以前的消费或者回溯消费。 public static void main(String[] args) { Properties properties = initConfig(); KafkaConsumer consumer = new KafkaConsumer<>(properties); consumer.subscribe(Arrays.asList(TOPIC_NAME)); // timeout参数设置多少合适？太短会使分区分配失败，太长又有可能造成一些不必要的等待 consumer.poll(Duration.ofMillis(2000)); // 获取消费者所分配到的分区 Set assignment = consumer.assignment(); System.out.println(assignment); for (TopicPartition tp : assignment) { // 参数partition表示分区，offset表示指定从分区的哪个位置开始消费 consumer.seek(tp, 10); } while (true) { ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord record : records) { System.out.println(record.offset() + \":\" + record.value()); } } } 如果没有获取到分区，则继续获取分区，知道获取到 public static void main(String[] args) { Properties properties = initConfig(); KafkaConsumer consumer = new KafkaConsumer<>(properties); consumer.subscribe(Arrays.asList(TOPIC_NAME)); // timeout参数设置多少合适？太短会使分区分配失败，太长又有可能造成一些不必要的等待 consumer.poll(Duration.ofMillis(2000)); // 获取消费者所分配到的分区 Set assignment = consumer.assignment(); System.out.println(assignment); // 如果没有获取到分区，则继续获取分区 while (assignment.size() == 0) { consumer.poll(Duration.ofMillis(100)); assignment = consumer.assignment(); } for (TopicPartition tp : assignment) { // 参数partition表示分区，offset表示指定从分区的哪个位置开始消费 consumer.seek(tp, 10); } while (true) { ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord record : records) { System.out.println(record.offset() + \":\" + record.value()); } } } 指定从分区末尾开始消费 // 从指定分区末尾开始消费 Map offsets = consumer.endOffsets(assignment); for (TopicPartition tp : assignment) { // 参数partition表示分区，offset表示指定从分区的哪个位置开始消费 consumer.seek(tp, offsets.get(tp)); } 演示位移越界操作，修改代码如下： // 从指定分区末尾开始消费 Map offsets = consumer.endOffsets(assignment); for (TopicPartition tp : assignment) { // 参数partition表示分区，offset表示指定从分区的哪个位置开始消费 consumer.seek(tp, offsets.get(tp) + 1); } 会通过auto offset reset参数的默认值将位置重置。效果如下 3.2.5 再均衡监听器 再均衡是指分区的所属从一个消费者转移到另一个消费者的行为，它为消费者组具备了高可用性和伸缩性提供了保障，使得我们既方便又安全地删除消费组内的消费者或者往消费组内添加消费者。不过再均衡发生期间，消费者是无法拉取消息的。 private static final AtomicBoolean isRunning = new AtomicBoolean(true); public static void main(String[] args) { Properties properties = initConfig(); KafkaConsumer consumer = new KafkaConsumer<>(properties); Map currentOffsets = new HashMap<>(); consumer.subscribe(Arrays.asList(TOPIC_NAME), new ConsumerRebalanceListener() { @Override public void onPartitionsRevoked(Collection partitions) { // 尽量避免重复消费 consumer.commitSync(currentOffsets); } @Override public void onPartitionsAssigned(Collection partitions) { } }); try { while (isRunning.get()) { ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for(ConsumerRecord record: records) { System.out.println(record.offset() + \":\" + record.value()); // 异步提交消费位移，再发生在均衡动作之前可以通过再均衡监听器的oonPartitionsRevoked回调执行commitSync()方法 currentOffsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1)); } consumer.commitAsync(currentOffsets, null); } } finally { consumer.close(); } } 3.2.6 消费者监听器 之前章节讲了生产者拦截器，对应的消费者也有相应的拦截器概念，消费者拦截器主要实在消费到消息或者在提交消费位移时进行的一些定制化的操作。 使用场景 对消费者消息设置了一个有效期的属性，如果某条消息在既定的时间窗口内无法到达，那就视为无效，不需要再被处理。 powered by Gitbook该文件修订时间： 2020-06-30 23:30:00 "}}